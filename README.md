# Text-Generation-Word-Level-Neural-Language-Model

Based on the words that have previously been seen in the sequence, a language model can forecast the likelihood of the subsequent word in the sequence.

Because they can employ a distributed representation, in which words with similar meanings have similar representations, and because they can use a vast context of recently observed words when generating predictions, neural network models are the technique of choice for creating statistical language models.

This notebook will be able:

* To prepare text for creating a language model based on words.
* To create and fit a neural language model using an LSTM hidden layer and a learnt embedding.
* To create new text using the learnt language model that shares the original text's statistical characteristics.


<img width="594" alt="Screenshot 2022-12-26 at 11 03 29 AM" src="https://user-images.githubusercontent.com/32131585/209507041-22a0a35d-f7df-4136-8688-49c778059404.png">
