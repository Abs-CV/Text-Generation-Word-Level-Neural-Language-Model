{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abs-CV/Text-Generation-Word-Level-Neural-Language-Model/blob/main/Word_Level_Neural_Language_Model_to_Generate_Text_PLATO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ckeZa08pysQa"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSyPkVztzI-4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "7da407b2-2b15-4482-fcf0-46a781db63a4"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-66fa059e-4236-4c14-8452-fd2d0551e027\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-66fa059e-4236-4c14-8452-fd2d0551e027\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving republic.txt to republic.txt\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wIuu0yvqzZrj"
      },
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "  file = open(filename,'r')\n",
        "  text=file.read()\n",
        "  file.close()\n",
        "  return(text)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "98whNrYy0UFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c1b4ae-3dce-4378-950e-e823519ae1cd"
      },
      "cell_type": "code",
      "source": [
        "in_filename='republic.txt'\n",
        "doc=load_doc(in_filename)\n",
        "print(doc[:200])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK I.\n",
            "\n",
            "\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
            "Artemis.); and also because I wanted to see in wha\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "OEWNvp_E0gsH"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def clean_doc(doc):\n",
        "  #replace '--' with a space\n",
        "  doc = doc.replace('--',' ')\n",
        "  #split into token with white spaces\n",
        "  tokens = doc.split()\n",
        "  #remove punctuation from each string\n",
        "  table=str.maketrans('','',string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  #remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  #make lower case\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_7F4q4B72btN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e98ced4-d274-4658-ba86-0119ed4438c2"
      },
      "cell_type": "code",
      "source": [
        "#clean document\n",
        "tokens=clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print(len(tokens),len(set(tokens)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said', 'to']\n",
            "117342 7323\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "SFogsg7h4Ez2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2730a7cc-33e6-4c40-c477-7f55c59fcd99"
      },
      "cell_type": "code",
      "source": [
        "length =50+1\n",
        "sequences=list()\n",
        "for i in range(length,len(tokens)):\n",
        "  #select sequence of tokens\n",
        "  seq=tokens[i-length:i]\n",
        "  #convert it to a line\n",
        "  line = ' '.join(seq)\n",
        "  #store\n",
        "  sequences.append(line)\n",
        "\n",
        "len(sequences),sequences[0] , sequences[1] "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(117291,\n",
              " 'book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "x0SywzIf6HG_"
      },
      "cell_type": "code",
      "source": [
        "def save_doc(lines,filename):\n",
        "  data='\\n'.join(lines)\n",
        "  file = open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ryQGm7Tu6-ib"
      },
      "cell_type": "code",
      "source": [
        "out_file='writi.txt'\n",
        "save_doc(sequences,out_file)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Vul8M147Nmf"
      },
      "cell_type": "code",
      "source": [
        "lines=sequences[:]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NRc522jGDo6v"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "M74vIPWPzg_n",
        "outputId": "0d0a82dd-4787-498e-d0b5-c156192514c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "vn6_lnKvEtSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dad07f3-bd85-4d48-8c03-d12fb563fef3"
      },
      "cell_type": "code",
      "source": [
        "vocab_size=len(tokenizer.word_index)+1\n",
        "vocab_size"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7324"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "sKf3nX7wMyTT"
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z-fjeLIBKDEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82e5ab0-06a0-4115-83ef-d56db5e5d929"
      },
      "cell_type": "code",
      "source": [
        "#seperate into input and output\n",
        "from keras.utils import to_categorical\n",
        "sequences = np.array(sequences)\n",
        "X,y=sequences[:,:-1],sequences[:,-1]\n",
        "y= to_categorical(y,num_classes=vocab_size)\n",
        "seq_length = X.shape[1]\n",
        "seq_length"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[:,:-1].shape,y.shape,X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXQWL6YNz1jh",
        "outputId": "6b8aa0a6-865f-4394-8728-315c0c031e56"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((117291, 50), (117291, 7324), (117291, 50))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "YIM5T5SXLBdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f35addf7-9fa5-4701-8d06-ff59e370c18c"
      },
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 50)            366200    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 100)           60400     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7324)              739724    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,256,824\n",
            "Trainable params: 1,256,824\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "XOHeGDVfL7OH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec9986fa-ff2c-4974-ffc8-0160e894869c"
      },
      "cell_type": "code",
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "917/917 [==============================] - 20s 14ms/step - loss: 6.1427 - accuracy: 0.0739\n",
            "Epoch 2/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 5.6857 - accuracy: 0.1077\n",
            "Epoch 3/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 5.4429 - accuracy: 0.1315\n",
            "Epoch 4/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 5.2884 - accuracy: 0.1455\n",
            "Epoch 5/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 5.1801 - accuracy: 0.1544\n",
            "Epoch 6/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 5.0873 - accuracy: 0.1598\n",
            "Epoch 7/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 5.0743 - accuracy: 0.1588\n",
            "Epoch 8/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 4.9518 - accuracy: 0.1686\n",
            "Epoch 9/100\n",
            "917/917 [==============================] - 13s 15ms/step - loss: 4.8534 - accuracy: 0.1753\n",
            "Epoch 10/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 4.7594 - accuracy: 0.1804\n",
            "Epoch 11/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 4.6693 - accuracy: 0.1858\n",
            "Epoch 12/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 4.5821 - accuracy: 0.1897\n",
            "Epoch 13/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 4.4988 - accuracy: 0.1943\n",
            "Epoch 14/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 4.4506 - accuracy: 0.1958\n",
            "Epoch 15/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 4.3634 - accuracy: 0.1987\n",
            "Epoch 16/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 4.2925 - accuracy: 0.2024\n",
            "Epoch 17/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 4.2397 - accuracy: 0.2051\n",
            "Epoch 18/100\n",
            "917/917 [==============================] - 15s 16ms/step - loss: 4.1628 - accuracy: 0.2087\n",
            "Epoch 19/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 4.0986 - accuracy: 0.2121\n",
            "Epoch 20/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 4.0345 - accuracy: 0.2165\n",
            "Epoch 21/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.9766 - accuracy: 0.2201\n",
            "Epoch 22/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.9235 - accuracy: 0.2247\n",
            "Epoch 23/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.8687 - accuracy: 0.2290\n",
            "Epoch 24/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 3.8182 - accuracy: 0.2337\n",
            "Epoch 25/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.7697 - accuracy: 0.2376\n",
            "Epoch 26/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.7229 - accuracy: 0.2420\n",
            "Epoch 27/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.6773 - accuracy: 0.2464\n",
            "Epoch 28/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.6366 - accuracy: 0.2503\n",
            "Epoch 29/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 3.5964 - accuracy: 0.2551\n",
            "Epoch 30/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 3.7012 - accuracy: 0.2480\n",
            "Epoch 31/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.6328 - accuracy: 0.2537\n",
            "Epoch 32/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.5403 - accuracy: 0.2617\n",
            "Epoch 33/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.4841 - accuracy: 0.2681\n",
            "Epoch 34/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.4284 - accuracy: 0.2740\n",
            "Epoch 35/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.3884 - accuracy: 0.2794\n",
            "Epoch 36/100\n",
            "917/917 [==============================] - 15s 16ms/step - loss: 3.3543 - accuracy: 0.2838\n",
            "Epoch 37/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.3187 - accuracy: 0.2888\n",
            "Epoch 38/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 3.2845 - accuracy: 0.2935\n",
            "Epoch 39/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 3.2502 - accuracy: 0.2992\n",
            "Epoch 40/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.2227 - accuracy: 0.3019\n",
            "Epoch 41/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 3.1891 - accuracy: 0.3072\n",
            "Epoch 42/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.1594 - accuracy: 0.3106\n",
            "Epoch 43/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.1249 - accuracy: 0.3155\n",
            "Epoch 44/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.0909 - accuracy: 0.3216\n",
            "Epoch 45/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.0641 - accuracy: 0.3253\n",
            "Epoch 46/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.0331 - accuracy: 0.3303\n",
            "Epoch 47/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 3.0020 - accuracy: 0.3349\n",
            "Epoch 48/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.9722 - accuracy: 0.3399\n",
            "Epoch 49/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.9472 - accuracy: 0.3440\n",
            "Epoch 50/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.9481 - accuracy: 0.3452\n",
            "Epoch 51/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.9087 - accuracy: 0.3510\n",
            "Epoch 52/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.8730 - accuracy: 0.3560\n",
            "Epoch 53/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 2.8364 - accuracy: 0.3616\n",
            "Epoch 54/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.8095 - accuracy: 0.3652\n",
            "Epoch 55/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.7842 - accuracy: 0.3684\n",
            "Epoch 56/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.7581 - accuracy: 0.3744\n",
            "Epoch 57/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.7324 - accuracy: 0.3780\n",
            "Epoch 58/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.7089 - accuracy: 0.3820\n",
            "Epoch 59/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.6839 - accuracy: 0.3867\n",
            "Epoch 60/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.6594 - accuracy: 0.3897\n",
            "Epoch 61/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.6333 - accuracy: 0.3933\n",
            "Epoch 62/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.6113 - accuracy: 0.3993\n",
            "Epoch 63/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.5891 - accuracy: 0.4036\n",
            "Epoch 64/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.5640 - accuracy: 0.4055\n",
            "Epoch 65/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 2.5405 - accuracy: 0.4113\n",
            "Epoch 66/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.5193 - accuracy: 0.4143\n",
            "Epoch 67/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.4989 - accuracy: 0.4176\n",
            "Epoch 68/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.4769 - accuracy: 0.4215\n",
            "Epoch 69/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.4526 - accuracy: 0.4259\n",
            "Epoch 70/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.4313 - accuracy: 0.4307\n",
            "Epoch 71/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.4115 - accuracy: 0.4333\n",
            "Epoch 72/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.3910 - accuracy: 0.4370\n",
            "Epoch 73/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.3714 - accuracy: 0.4414\n",
            "Epoch 74/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.3531 - accuracy: 0.4448\n",
            "Epoch 75/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.3342 - accuracy: 0.4478\n",
            "Epoch 76/100\n",
            "917/917 [==============================] - 13s 15ms/step - loss: 2.3146 - accuracy: 0.4528\n",
            "Epoch 77/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.2961 - accuracy: 0.4558\n",
            "Epoch 78/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.2766 - accuracy: 0.4596\n",
            "Epoch 79/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.2535 - accuracy: 0.4641\n",
            "Epoch 80/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.2382 - accuracy: 0.4673\n",
            "Epoch 81/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.2223 - accuracy: 0.4687\n",
            "Epoch 82/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.2034 - accuracy: 0.4749\n",
            "Epoch 83/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.1862 - accuracy: 0.4756\n",
            "Epoch 84/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.1723 - accuracy: 0.4790\n",
            "Epoch 85/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.1527 - accuracy: 0.4834\n",
            "Epoch 86/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.1338 - accuracy: 0.4864\n",
            "Epoch 87/100\n",
            "917/917 [==============================] - 13s 15ms/step - loss: 2.1225 - accuracy: 0.4887\n",
            "Epoch 88/100\n",
            "917/917 [==============================] - 13s 15ms/step - loss: 2.1039 - accuracy: 0.4934\n",
            "Epoch 89/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.0818 - accuracy: 0.4985\n",
            "Epoch 90/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.0706 - accuracy: 0.5008\n",
            "Epoch 91/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.0551 - accuracy: 0.5040\n",
            "Epoch 92/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.0381 - accuracy: 0.5069\n",
            "Epoch 93/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.0273 - accuracy: 0.5080\n",
            "Epoch 94/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 2.0089 - accuracy: 0.5120\n",
            "Epoch 95/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 1.9937 - accuracy: 0.5155\n",
            "Epoch 96/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 1.9763 - accuracy: 0.5192\n",
            "Epoch 97/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 1.9688 - accuracy: 0.5206\n",
            "Epoch 98/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 1.9555 - accuracy: 0.5234\n",
            "Epoch 99/100\n",
            "917/917 [==============================] - 14s 15ms/step - loss: 1.9366 - accuracy: 0.5266\n",
            "Epoch 100/100\n",
            "917/917 [==============================] - 13s 14ms/step - loss: 1.9271 - accuracy: 0.5284\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3380e03f70>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "qyZZK49D4UuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the tokenizer\n",
        "from pickle import dump\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "SMpbz8_S8Slg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = np.argmax(model.predict(encoded), axis=1)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)"
      ],
      "metadata": {
        "id": "o2vVx1r99joV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# generate a sequence from a language model\n",
        "\n",
        "\n",
        "# load cleaned text sequences\n",
        "in_filename = 'writi.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "seq_length = len(lines[0].split()) - 1\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "Hn1Ye9LW46Gj",
        "outputId": "a4e4358c-91ff-4dab-8700-bfc58738f49d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make us look downwards and not upwards what do you mean he asked you i replied have in your mind a truly sublime conception of our knowledge of the things above and i dare say that if a person were to throw his head back and study the fretted ceiling you\n",
            "\n",
            "1/1 [==============================] - 1s 610ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "would still think that the just man is derived from the object which they require who virtuous will profit thither in the same time highspirited and powerful of them poetry boldly intelligence but the inharmonious is cowardly and boorish very true and when a man feel right he replied you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0aqWYrC8Ve2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZ8tJA19-X1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}